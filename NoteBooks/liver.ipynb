{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1845456,"sourceType":"datasetVersion","datasetId":1018259}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U imbalanced-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:21:17.734043Z","iopub.execute_input":"2025-06-30T06:21:17.734765Z","iopub.status.idle":"2025-06-30T06:21:21.475857Z","shell.execute_reply.started":"2025-06-30T06:21:17.734723Z","shell.execute_reply":"2025-06-30T06:21:21.474601Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\nRequirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.26.4)\nRequirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.2)\nRequirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\nRequirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\nRequirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.24.3->imbalanced-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.24.3->imbalanced-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.24.3->imbalanced-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.24.3->imbalanced-learn) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.24.3->imbalanced-learn) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.24.3->imbalanced-learn) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.24.3->imbalanced-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.24.3->imbalanced-learn) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.24.3->imbalanced-learn) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.24.3->imbalanced-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.24.3->imbalanced-learn) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nliver_disease_improved.py\n\nA self-contained script that:\n1. Loads & cleans the liver disease dataset\n2. Splits into train & test (no leakage)\n3. Imputes missing values (median) & scales features (MinMax)\n4. Manually oversamples the minority class in train\n5. Runs RandomizedSearchCV on XGBoost (n_jobs=1 to avoid pickling errors)\n6. Fits a final XGBoost model with early stopping\n7. Evaluates on the hold-out test set\n8. Saves the model, imputer, and scaler for later use\n\"\"\"\n\nimport os\nimport warnings\nimport joblib\n\nimport pandas as pd\nfrom sklearn.model_selection import (\n    train_test_split,\n    StratifiedKFold,\n    RandomizedSearchCV,\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.utils import resample\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    classification_report,\n    roc_auc_score,\n)\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef main():\n    # 1) Load & clean\n    DATA_PATH = \"../input/liver-disease-patient-dataset/Liver Patient Dataset (LPD)_train.csv\"\n    df = pd.read_csv(DATA_PATH, encoding=\"unicode_escape\")\n    df = df.fillna(method=\"bfill\").drop_duplicates()\n    df[\"Result\"] = df[\"Result\"].map({1: 0, 2: 1})\n    df[\"Gender of the patient\"] = df[\"Gender of the patient\"].map({\"Female\": 0, \"Male\": 1})\n\n    # 2) Train/test split (no leakage!)\n    X = df.drop(\"Result\", axis=1)\n    y = df[\"Result\"]\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.20, stratify=y, random_state=42\n    )\n\n    # 3) Median imputation + MinMax scaling (fit on train only)\n    imputer = SimpleImputer(strategy=\"median\")\n    X_train_imp = pd.DataFrame(\n        imputer.fit_transform(X_train),\n        columns=X_train.columns,\n        index=X_train.index,\n    )\n    X_test_imp = pd.DataFrame(\n        imputer.transform(X_test),\n        columns=X_test.columns,\n        index=X_test.index,\n    )\n\n    scaler = MinMaxScaler()\n    X_train_scaled = pd.DataFrame(\n        scaler.fit_transform(X_train_imp),\n        columns=X_train_imp.columns,\n        index=X_train_imp.index,\n    )\n    X_test_scaled = pd.DataFrame(\n        scaler.transform(X_test_imp),\n        columns=X_test_imp.columns,\n        index=X_test_imp.index,\n    )\n\n    # 4) Manual oversampling of minority class in TRAIN\n    train_bal = pd.concat([X_train_scaled, y_train.rename(\"Result\")], axis=1)\n    majority = train_bal[train_bal.Result == 0]\n    minority = train_bal[train_bal.Result == 1]\n    minority_upsampled = resample(\n        minority,\n        replace=True,\n        n_samples=len(majority),\n        random_state=42,\n    )\n    train_balanced = pd.concat([majority, minority_upsampled])\n    X_train_bal = train_balanced.drop(\"Result\", axis=1)\n    y_train_bal = train_balanced[\"Result\"]\n\n    # 5) Hyperparameter search on XGBoost (n_jobs=1 avoids pickling issues)\n    base_xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n    param_dist = {\n        \"n_estimators\":     [100, 200, 500],\n        \"max_depth\":        [3, 5, 7],\n        \"learning_rate\":    [0.01, 0.1, 0.2],\n        \"subsample\":        [0.6, 0.8, 1.0],\n        \"colsample_bytree\": [0.6, 0.8, 1.0],\n    }\n    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n    search = RandomizedSearchCV(\n        base_xgb,\n        param_distributions=param_dist,\n        n_iter=20,\n        scoring=\"roc_auc\",\n        cv=cv,\n        n_jobs=1,             # ← avoid BrokenProcessPool errors\n        verbose=1,\n        random_state=42,\n    )\n    print(\"Starting hyperparameter search…\")\n    search.fit(X_train_bal, y_train_bal)\n    print(f\"Best CV ROC-AUC: {search.best_score_:.4f}\")\n    print(\"Best params:\", search.best_params_)\n\n    # 6) Final model with early stopping\n    best_params = search.best_params_\n    final_model = XGBClassifier(\n        **best_params,\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n        early_stopping_rounds=10,\n        random_state=42,\n    )\n    final_model.fit(\n        X_train_bal,\n        y_train_bal,\n        eval_set=[(X_test_scaled, y_test)],\n        verbose=False,\n    )\n\n    # 7) Evaluate on test set\n    y_pred = final_model.predict(X_test_scaled)\n    y_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n    print(\"\\n=== Test Set Performance ===\")\n    print(\"Accuracy       :\", accuracy_score(y_test, y_pred))\n    print(\"ROC-AUC        :\", roc_auc_score(y_test, y_proba))\n    print(\"Confusion Mat. :\\n\", confusion_matrix(y_test, y_pred))\n    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n    # 8) Save artifacts\n    ARTIFACT_DIR = \"artifacts\"\n    os.makedirs(ARTIFACT_DIR, exist_ok=True)\n    joblib.dump(final_model, os.path.join(ARTIFACT_DIR, \"xgb_model.joblib\"))\n    joblib.dump(imputer, os.path.join(ARTIFACT_DIR, \"imputer.joblib\"))\n    joblib.dump(scaler, os.path.join(ARTIFACT_DIR, \"scaler.joblib\"))\n    print(f\"\\nSaved model, imputer & scaler → {ARTIFACT_DIR}/\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:21:21.477818Z","iopub.execute_input":"2025-06-30T06:21:21.47809Z","iopub.status.idle":"2025-06-30T06:21:46.714584Z","shell.execute_reply.started":"2025-06-30T06:21:21.478064Z","shell.execute_reply":"2025-06-30T06:21:46.713685Z"}},"outputs":[{"name":"stdout","text":"Starting hyperparameter search…\nFitting 3 folds for each of 20 candidates, totalling 60 fits\nBest CV ROC-AUC: 0.9999\nBest params: {'subsample': 1.0, 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.2, 'colsample_bytree': 0.8}\n\n=== Test Set Performance ===\nAccuracy       : 0.9950235725510739\nROC-AUC        : 0.9995433739472558\nConfusion Mat. :\n [[2714    7]\n [  12 1085]]\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      2721\n           1       0.99      0.99      0.99      1097\n\n    accuracy                           1.00      3818\n   macro avg       0.99      0.99      0.99      3818\nweighted avg       1.00      1.00      1.00      3818\n\n\nSaved model, imputer & scaler → artifacts/\n","output_type":"stream"}],"execution_count":8}]}